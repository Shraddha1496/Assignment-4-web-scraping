{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Assignment 4 - Web Scraping**\n",
    "\n",
    "1. Scrape the details of most viewed videos on YouTube from Wikipedia:<br>\n",
    "Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos/<br>\n",
    "You need to find following details:<br>\n",
    "A) Rank<br>\n",
    "B) Name<br>\n",
    "C) Artist<br>\n",
    "D) Upload date<br>\n",
    "E) Views<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required libraries\n",
    "import pandas as pd\n",
    "import time\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connecting to the web driver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "time.sleep(3)\n",
    "\n",
    "#get the url\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "driver.get(url)\n",
    "\n",
    "#creating empty list for scraping data\n",
    "Rank=[]\n",
    "Name=[]\n",
    "Artist=[]\n",
    "Upload_date=[]\n",
    "Views=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping Rank\n",
    "rank = driver.find_elements_by_xpath(\"//td[@align='center']\")[:15]\n",
    "for i in rank:       \n",
    "    if i.text is None :\n",
    "        Rank.append(\"--\") \n",
    "    else:\n",
    "        Rank.append(i.text)        \n",
    "\n",
    "time.sleep(3)        \n",
    "# scraping Name\n",
    "name = driver.find_elements_by_xpath(\"//table[@class='wikitable sortable jquery-tablesorter']//a\")[:20]\n",
    "for i in name:       \n",
    "    if i.text is None :\n",
    "        Name.append(\"--\") \n",
    "    else:\n",
    "        Name.append(i.text)\n",
    "\n",
    "time.sleep(3)        \n",
    "# scraping Artist\n",
    "artist = driver.find_elements_by_xpath(\"//table[@class='wikitable sortable jquery-tablesorter']//a\")[:20]\n",
    "for i in artist:\n",
    "    if i.text is None :\n",
    "        Artist.append(\"--\") \n",
    "    else:\n",
    "        Artist.append(i.text)        \n",
    "        \n",
    "time.sleep(3)\n",
    "# scraping Upload_date\n",
    "date = driver.find_elements_by_xpath(\"//td[@align='right']\")[:5]\n",
    "for i in date:       \n",
    "    if i.text is None :\n",
    "        Upload_date.append(\"--\") \n",
    "    else:\n",
    "        Upload_date.append(i.text)        \n",
    "        \n",
    "time.sleep(3)        \n",
    "# scraping Views\n",
    "view = driver.find_elements_by_xpath(\"//td[@align='center']\")[:15]\n",
    "for i in view:       \n",
    "        if i.text is None:\n",
    "            Views.append(\"--\")\n",
    "        else:\n",
    "            Views.append(i.text) \n",
    "            \n",
    "# creating the dataframe from the scraped data\n",
    "df=pd.DataFrame({\"Rank\":Rank[0::3],\"Name\":Name[0::4],\"Artist\":Artist[2::4],\"Upload_date\":Upload_date,\"Views\":Views[1::3]})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Scrape the details team India’s international fixtures from bcci.tv.<br>\n",
    "Url = https://www.bcci.tv/.<br>\n",
    "You need to find following details:<br>\n",
    "A) Match title (I.e. 1st ODI)<br>\n",
    "B) Series<br>\n",
    "C) Place<br>\n",
    "D) Date<br>\n",
    "E) Time<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Chrome webdriver\n",
    "driver = webdriver.Chrome('chromedriver.exe')\n",
    "# opening the url\n",
    "driver.get('https://www.bcci.tv')\n",
    "\n",
    "# getiing the internation fixture page\n",
    "page = driver.find_element_by_xpath('/html/body/footer/div/nav/ul/li[2]/ul/li[1]/a')\n",
    "page.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing details\n",
    "title = []\n",
    "series = []\n",
    "place = []\n",
    "date = []\n",
    "time = []\n",
    "list1 = []\n",
    "list2 = []\n",
    "\n",
    "# scraping the title\n",
    "for i in driver.find_elements_by_xpath(\"//span[@class='u-unskewed-text fixture__format']\"):\n",
    "    title.append(i.text)\n",
    "\n",
    "# scraping series name\n",
    "for j in driver.find_elements_by_xpath(\"//span[@class='u-unskewed-text fixture__tournament-label u-truncated']\"):\n",
    "    series.append(j.text)\n",
    "    \n",
    "# scraping the place\n",
    "for k in driver.find_elements_by_xpath(\"//p[@class='fixture__additional-info']/span\"):\n",
    "    place.append(k.text)\n",
    "    \n",
    "# scraping date of the match\n",
    "for l in driver.find_elements_by_xpath(\"//span[@class='fixture__date']\"):\n",
    "    list1.append(l.text)\n",
    "for m in driver.find_elements_by_xpath(\"//span[@class='fixture__month']\"):\n",
    "    list2.append(m.text)\n",
    "date = [i + j for i, j in zip(list1, list2)]\n",
    "\n",
    "# scraping time\n",
    "for n in driver.find_elements_by_xpath(\"//span[@class='fixture__time']\"):\n",
    "    time.append(n.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixture = pd.DataFrame({})\n",
    "fixture['Match Title'] = title\n",
    "fixture['Series'] = series\n",
    "fixture['Place'] = place\n",
    "fixture['Date'] = date\n",
    "fixture['Time'] = time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Scrape the details of selenium exception from guru99.com.<br>\n",
    "Url = https://www.guru99.com/<br>\n",
    "You need to find following details:<br>\n",
    "A) Name<br>\n",
    "B) Description<br>\n",
    "Note: - From guru99 home page you have to reach to selenium exception handling page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Chrome webdriver\n",
    "driver = webdriver.Chrome('chromedriver.exe')\n",
    "# opening url\n",
    "driver.get('https://www.guru99.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locating search input\n",
    "search_inp = input(\"Enter the topic: \")\n",
    "serach_bar = driver.find_element_by_xpath('//*[@id=\"gsc-i-id2\"]')\n",
    "serach_bar.send_keys(search_inp)\n",
    "driver.find_element_by_xpath('//*[@id=\"___gcse_1\"]/div/div/form/table/tbody/tr/td[2]/button').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the first url from the search list\n",
    "a = driver.find_element_by_xpath('//*[@id=\"___gcse_1\"]/div/div/div[1]/div[6]/div[2]/div/div/div[1]/div[1]/div[1]/div[1]/div/a')\n",
    "driver.get(a.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping details\n",
    "name = []\n",
    "desc = []\n",
    "\n",
    "# scraping the exception name\n",
    "for i in driver.find_elements_by_xpath('//*[@id=\"g-mainbar\"]/div[1]/div/div/div/div/div/div[2]/table/tbody/tr/td[1]'):\n",
    "    name.append(i.text)\n",
    "\n",
    "#scraping the description\n",
    "for j in driver.find_elements_by_xpath('//*[@id=\"g-mainbar\"]/div[1]/div/div/div/div/div/div[2]/table/tbody/tr/td[2]'):\n",
    "    desc.append(j.text)\n",
    "\n",
    "name = name[1:]\n",
    "desc = desc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Exception = pd.DataFrame({})\n",
    "Exception['Exception Name'] = name\n",
    "Exception['Description'] = desc\n",
    "Exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Scrape the details of State-wise GDP of India from statisticstime.com<br>\n",
    "Url = http://statisticstimes.com/<br>\n",
    "You have to find following details:<br>\n",
    "A) Rank<br>\n",
    "B) State<br>\n",
    "C) GSDP at current price (19-20)<br>\n",
    "D) GSDP at current price (18-19)<br>\n",
    "E) Share(18-19)<br>\n",
    "F) GDP($ billion)<br>\n",
    "Note: - From statisticstimes home page you have to reach to economy page through code.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connecting to the web driver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "\n",
    "#get the url\n",
    "url = \"http://statisticstimes.com/\"\n",
    "driver.get(url)\n",
    "\n",
    "# clicking the 'economy'\n",
    "economy=driver.find_element_by_xpath(\"/html/body/div[2]/div[1]/div[2]/div[2]/button\")\n",
    "economy.click()\n",
    "\n",
    "# clicking the 'dropdown button'\n",
    "dropdown=driver.find_element_by_xpath(\"/html/body/div[2]/div[1]/div[2]/div[2]/div/a[3]\")\n",
    "dropdown.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking the 'statewise GDP of India'\n",
    "GDP_of_India=driver.find_element_by_xpath(\"/html/body/div[2]/div[2]/div[2]/ul/li[1]/a\")\n",
    "GDP_of_India.click()\n",
    "\n",
    "#creating empty list for scraping data\n",
    "Rank=[]\n",
    "State=[]\n",
    "GDSP_19_20=[]\n",
    "GSDP_18_19=[]\n",
    "Share_18_19=[]\n",
    "GDP_billion=[]\n",
    "\n",
    "# scraping rank\n",
    "rank = driver.find_elements_by_xpath(\"//td[@class='data1']\")[:33]\n",
    "for i in rank:       \n",
    "    if i.text is None :\n",
    "        Rank.append(\"--\") \n",
    "    else:\n",
    "        Rank.append(i.text)\n",
    "                \n",
    "# scraping state\n",
    "state = driver.find_elements_by_xpath(\"//td[@class='name']\")[:33]\n",
    "for i in state:       \n",
    "    if i.text is None :\n",
    "        State.append(\"--\") \n",
    "    else:\n",
    "        State.append(i.text)\n",
    "        \n",
    "# scraping GSDP at current price (19-20)\n",
    "GSDP = driver.find_elements_by_xpath(\"//td[@class='data']\")[:165]\n",
    "for i in GSDP:       \n",
    "    if i.text is None :\n",
    "        GDSP_19_20.append(\"--\") \n",
    "    else:\n",
    "        GDSP_19_20.append(i.text)\n",
    "                \n",
    "# scraping GSDP at current price (18-19)\n",
    "GSDP = driver.find_elements_by_xpath(\"//td[@class='data sorting_1']\")[:33]\n",
    "for i in GSDP:       \n",
    "    if i.text is None :\n",
    "        GSDP_18_19.append(\"--\") \n",
    "    else:\n",
    "        GSDP_18_19.append(i.text)\n",
    "                \n",
    "# scraping Share(18-19)\n",
    "share = driver.find_elements_by_xpath(\"//td[@class='data']\")[:165]\n",
    "for i in share:       \n",
    "    if i.text is None :\n",
    "        Share_18_19.append(\"--\") \n",
    "    else:\n",
    "        Share_18_19.append(i.text)\n",
    "        \n",
    "# scraping GDP($ billion)\n",
    "GDP = driver.find_elements_by_xpath(\"//td[@class='data']\")[:165]\n",
    "for i in GDP:\n",
    "    if i.text is None :\n",
    "            GDP_billion.append(\"--\") \n",
    "    else:\n",
    "        GDP_billion.append(i.text)\n",
    "        \n",
    "# creating the dataframe from the scraped data\n",
    "df=pd.DataFrame({\"Rank\":Rank,\"State\":State,\"GDSP_19_20\":GDSP_19_20[::5],\"GSDP_18_19\":GSDP_18_19,\"Share_18_19\":Share_18_19[1::5],\"GDP_billion\":GDP_billion[2::5]})\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Scrape the details of trending repositories on Github.com<br>\n",
    "Url = https://github.com/<br>\n",
    "You have to find the following details:<br>\n",
    "A) Repository title<br>\n",
    "B) Repository description<br>\n",
    "C) Contributors count<br>\n",
    "D) Language used<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Chrome webdriver\n",
    "driver = webdriver.Chrome('chromedriver.exe')\n",
    "# opening the url\n",
    "driver.get('https://www.github.com')\n",
    "\n",
    "trending = driver.find_element_by_xpath('/html/body/div[1]/header/div/div[2]/nav/ul/li[4]/details/div/ul[2]/li[3]/a')\n",
    "trending = trending.get_attribute('href')\n",
    "driver.get(trending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vreating variable to store the data\n",
    "title = []\n",
    "desc = []\n",
    "contri = []\n",
    "lang = []\n",
    "url = []\n",
    "\n",
    "#scraping title\n",
    "for h in driver.find_elements_by_xpath(\"//h1[@class='h3 lh-condensed']/a\"):\n",
    "    title.append(h.text[h.text.find(\"/\")+1:])\n",
    "\n",
    "# sraping language used\n",
    "loop = range(1,26)\n",
    "for i in loop:\n",
    "    try:\n",
    "        a = driver.find_element_by_xpath('//*[@id=\"js-pjax-container\"]/div[3]/div/div[2]/article['+str(i)+']/div[2]/span[1]/span[2]')\n",
    "        lang.append(a.text)\n",
    "    except NoSuchElementException:\n",
    "        lang.append('-')\n",
    "\n",
    "# scraping contribution count\n",
    "for j in driver.find_elements_by_xpath(\"//div[@class='f6 color-text-secondary mt-2']/a[2]\"):\n",
    "    contri.append(j.text)\n",
    "    \n",
    "# scraping title and description\n",
    "for k in driver.find_elements_by_xpath(\"//h1[@class='h3 lh-condensed']/a\"):\n",
    "    url.append(k.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "github = pd.DataFrame({'Repository title': title,\n",
    "                      'Contributord count': contri,\n",
    "                      'Language Used': lang})\n",
    "github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Scrape the details of top 100 songs on billboard.com<br>\n",
    "Url = https://www.billboard.com/<br>\n",
    "You have to find the following details:<br>\n",
    "A) Song name<br>\n",
    "B) Artist name<br>\n",
    "C) Last week rank<br>\n",
    "D) Peak rank<br>\n",
    "E) Weeks on board<br>\n",
    "Note: - From the home page you have to click on the charts option then hot 100-page link through code.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connecting to the web driver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "\n",
    "#get the url\n",
    "url = \"https://www.billboard.com/\"\n",
    "driver.get(url)\n",
    "\n",
    "# clicking the chart\n",
    "chart=driver.find_element_by_xpath(\"/html/body/div[2]/div[2]/div[2]/header/div/ul/li[1]\")\n",
    "chart.click()\n",
    "\n",
    "# clicking the view chart\n",
    "hot100=driver.find_element_by_xpath(\"/html/body/main/div[2]/div/div[1]/a/div[2]/div[2]/div[1]\")\n",
    "hot100.click()\n",
    "\n",
    "#creating empty list for scraping data\n",
    "Song_name=[]\n",
    "Artist_name=[]\n",
    "Last_week_rank=[]\n",
    "Peak_rank=[]\n",
    "Weeks_on_board=[]\n",
    "\n",
    "# scraping Song name\n",
    "song = driver.find_elements_by_xpath(\"//span[@class='chart-element__information__song text--truncate color--primary']\")\n",
    "for i in song:       \n",
    "    if i.text is None :\n",
    "        Song_name.append(\"--\") \n",
    "    else:\n",
    "        Song_name.append(i.text)         \n",
    "\n",
    "# scraping Artist name\n",
    "artist = driver.find_elements_by_xpath(\"//span[@class='chart-element__information__artist text--truncate color--secondary']\")\n",
    "for i in artist:       \n",
    "    if i.text is None :\n",
    "        Artist_name.append(\"--\") \n",
    "    else:\n",
    "        Artist_name.append(i.text) \n",
    "        \n",
    "# scraping last week rank\n",
    "lw_rank = driver.find_elements_by_xpath(\"//div[@class='chart-element__meta text--center color--secondary text--last']\")\n",
    "for i in lw_rank:       \n",
    "    if i.text is None :\n",
    "        Last_week_rank.append(\"--\") \n",
    "    else:\n",
    "        Last_week_rank.append(i.text)  \n",
    "\n",
    "# scraping peak rank\n",
    "peak = driver.find_elements_by_xpath(\"//div[@class='chart-element__meta text--center color--secondary text--peak']\")\n",
    "for i in peak:       \n",
    "    if i.text is None :\n",
    "        Peak_rank.append(\"--\") \n",
    "    else:\n",
    "        Peak_rank.append(i.text)\n",
    "        \n",
    "# scraping Weeks_on_board\n",
    "on_board = driver.find_elements_by_xpath(\"//div[@class='chart-element__meta text--center color--secondary text--week']\")\n",
    "for i in on_board:       \n",
    "    if i.text is None :\n",
    "        Weeks_on_board.append(\"--\")\n",
    "    else:\n",
    "        Weeks_on_board.append(i.text)\n",
    "\n",
    "# creating the dataframe from the scraped data\n",
    "df=pd.DataFrame({\"Song_name\":Song_name,\"Artist_name\":Artist_name,\"Last_week_rank\":Last_week_rank,\"Peak_rank\":Peak_rank,\"Weeks_on_board\":Weeks_on_board})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Scrape the details of Data science recruiters from naukri.com<br>\n",
    "Url = https://www.naukri.com/<br>\n",
    "You have to find the following details:<br>\n",
    "A) Name<br>\n",
    "B) Designation<br>\n",
    "C) Company<br>\n",
    "D) Skills they hire for<br>\n",
    "E) Location<br>\n",
    "Note: - From naukri.com homepage click on the recruiters option and the on the search pane type Data science and \n",
    "click on search. All this should be done through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connecting to the web driver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "\n",
    "#get the url\n",
    "url = \"https://www.naukri.com/hr-recruiters-consultants\"\n",
    "driver.get(url)\n",
    "\n",
    "# entering “Data Analyst” in “Skill,Designations,Companies” field.\n",
    "search_field=driver.find_element_by_xpath(\"//input[@class='sugInp']\") #job search bar\n",
    "search_field.send_keys(\"Data Science\")\n",
    "\n",
    "# clicking the search button\n",
    "search_button=driver.find_element_by_id(\"qsbFormBtn\")\n",
    "search_button.click()\n",
    "\n",
    "#creating empty list for scraping data\n",
    "Name=[]\n",
    "Designation=[]\n",
    "Company=[]\n",
    "Skills_they_hire_for=[]\n",
    "Location=[]\n",
    "\n",
    "#scraping the job-titles\n",
    "title=driver.find_elements_by_xpath(\"//p[@class='highlightable']//a\")[:48]\n",
    "for i in title:\n",
    "    if i.text is None :\n",
    "        Name.append(\"--\") \n",
    "    else:\n",
    "        Name.append(i.text)\n",
    "        \n",
    "#scraping the Designation\n",
    "deg=driver.find_elements_by_xpath(\"//span[@class='ellipsis clr']\")[:48]\n",
    "for i in deg:\n",
    "    if i.text is None :\n",
    "        Designation.append(\"--\") \n",
    "    else:\n",
    "        Designation.append(i.text)\n",
    "               \n",
    "#scraping the Company\n",
    "comp=driver.find_elements_by_xpath(\"//a[@class='ellipsis']\")[:48]\n",
    "for i in comp:\n",
    "    if i.text is None :\n",
    "        Company.append(\"--\") \n",
    "    else:\n",
    "        Company.append(i.text)\n",
    "               \n",
    "#scraping the Skills_they_hire_for\n",
    "skills=driver.find_elements_by_xpath(\"//div[@class='hireSec highlightable']\")[:48]\n",
    "for i in skills:\n",
    "    if i.text is None :\n",
    "        Skills_they_hire_for.append(\"--\") \n",
    "    else:\n",
    "        Skills_they_hire_for.append(i.text)\n",
    "        \n",
    "#scraping the Location\n",
    "loc=driver.find_elements_by_xpath(\"//small[@class='ellipsis']\")[:48]\n",
    "for i in loc:\n",
    "    if i.text is None :\n",
    "        Location.append(\"--\") \n",
    "    else:\n",
    "        Location.append(i.text)  \n",
    "        \n",
    "# creating the dataframe from the scraped data\n",
    "df=pd.DataFrame({\"Name\":Name,\"Designation\":Designation,\"Company\":Company,\"Skills_they_hire_for\":Skills_they_hire_for,\"Location\":Location})\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Scrape the details of Highest selling novels.<br>\n",
    "Url = https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey\u0002compare/<br>\n",
    "You have to find the following details:<br>\n",
    "A) Book name<br>\n",
    "B) Author name<br>\n",
    "C) Volumes sold<br>\n",
    "D) Publisher<br>\n",
    "E) Genre<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connecting to the web driver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "\n",
    "#get the url\n",
    "url = \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare/\"\n",
    "driver.get(url)\n",
    "\n",
    "# creating empty lists for scraping data\n",
    "Book_name=[]\n",
    "Author_name=[]\n",
    "Volumes_sold=[]\n",
    "Publisher=[]\n",
    "Genre=[]\n",
    "\n",
    "#scraping Books Name\n",
    "book=driver.find_elements_by_xpath(\"//td[@class='left']\")\n",
    "for i in book:\n",
    "    if i.text is None :\n",
    "        Book_name.append(\"--\") \n",
    "    else:\n",
    "        Book_name.append(i.text)\n",
    "                \n",
    "# scraping Author name\n",
    "author=driver.find_elements_by_xpath(\"//td[@class='left']\")\n",
    "for i in author:\n",
    "    if i.text is None :\n",
    "        Author_name.append(\"--\") \n",
    "    else:\n",
    "        Author_name.append(i.text)\n",
    "        \n",
    "# scraping Volumes Sold\n",
    "volume=driver.find_elements_by_xpath(\"//td[@class='left']\")\n",
    "for i in volume:\n",
    "    if i.text is None :\n",
    "        Volumes_sold.append(\"--\") \n",
    "    else:\n",
    "        Volumes_sold.append(i.text)\n",
    "                \n",
    "# scraping Publisher\n",
    "publisher=driver.find_elements_by_xpath(\"//td[@class='left']\")\n",
    "for i in publisher:\n",
    "    if i.text is None :\n",
    "        Publisher.append(\"--\") \n",
    "    else:\n",
    "        Publisher.append(i.text)\n",
    "               \n",
    "# scraping Genre\n",
    "genre = driver.find_elements_by_xpath((\"//td[@class='last left']\"))\n",
    "for i in genre:\n",
    "    if i.text is None :\n",
    "        Genre.append(\"--\")\n",
    "    else:\n",
    "        Genre.append(i.text)\n",
    "        \n",
    "# creating the dataframe from the scraped data\n",
    "df=pd.DataFrame({\"Book_name\":Book_name[1::5],\"Author_name\":Author_name[2::5],\"Volumes_sold\":Volumes_sold[3::5],\"Publisher\":Publisher[4::5],\"Genre\":Genre})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Scrape the details most watched tv series of all time from imdb.com<br>\n",
    " Url = https://www.imdb.com/list/ls095964455/<br>\n",
    "You have to find the following details:<br>\n",
    "A) Name<br>\n",
    "B) Year span<br>\n",
    "C) Genre<br>\n",
    "D) Run time<br>\n",
    "E) Ratings<br>\n",
    "F) Votes<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connecting to the web driver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "\n",
    "#get the url\n",
    "url = \"https://www.imdb.com/list/ls095964455/\"\n",
    "driver.get(url)\n",
    "\n",
    "# creating empty lists for scraping data\n",
    "Name=[]\n",
    "Year_span=[]\n",
    "Genre=[]\n",
    "Run_time=[]\n",
    "Ratings=[]\n",
    "Votes=[]\n",
    "\n",
    "# scraping top Movies name\n",
    "name = driver.find_elements_by_xpath((\"//h3[@class='lister-item-header']//a\"))\n",
    "for i in name:       \n",
    "    if i.text is None :\n",
    "        Name.append(\"--\") \n",
    "    else:\n",
    "        Name.append(i.text)\n",
    "        \n",
    "#scraping year span       \n",
    "year = driver.find_elements_by_xpath((\"//span[@class='lister-item-year text-muted unbold']\"))\n",
    "for i in year:       \n",
    "    if i.text is None :\n",
    "        Year_span.append(\"--\") \n",
    "    else:\n",
    "        Year_span.append(i.text) \n",
    "        \n",
    "#scraping genre       \n",
    "gen = driver.find_elements_by_xpath((\"//span[@class='genre']\"))\n",
    "for i in gen :       \n",
    "    if i.text is None :\n",
    "        Genre.append(\"--\") \n",
    "    else:\n",
    "        Genre.append(i.text)\n",
    "        \n",
    "#scraping run time\n",
    "time = driver.find_elements_by_xpath((\"//span[@class='runtime']\"))\n",
    "for i in time:       \n",
    "    if i.text is None :\n",
    "        Run_time.append(\"--\") \n",
    "    else:\n",
    "        Run_time.append(i.text)\n",
    "\n",
    "#scraping ratings\n",
    "rate = driver.find_elements_by_xpath((\"//div[@class='ipl-rating-star small']\"))\n",
    "for i in rate:       \n",
    "    if i.text is None :\n",
    "        Ratings.append(\"--\") \n",
    "    else:\n",
    "        Ratings.append(i.text)\n",
    "\n",
    "#scraping votes\n",
    "vote = driver.find_elements_by_xpath((\"//span[@name='nv']\"))\n",
    "for i in vote:       \n",
    "    if i.text is None :\n",
    "        Votes.append(\"--\") \n",
    "    else:\n",
    "        Votes.append(i.text)\n",
    "\n",
    "# creating the dataframe from the scraped data\n",
    "df=pd.DataFrame({\"Name\":Name,\"Year_span\":Year_span,\"Genre\":Genre,\"Run_time\":Run_time,\"Ratings\":Ratings,\"Votes\":Votes})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Details of Datasets from UCI machine learning repositories.<br>\n",
    " Url = https://archive.ics.uci.edu/<br>\n",
    " You have to find the following details:<br>\n",
    "A) Dataset name<br>\n",
    "B) Data type<br>\n",
    "C) Task<br>\n",
    "D) Attribute type<br>\n",
    "E) No of instances<br>\n",
    "F) No of attribute<br>\n",
    "G) Year<br>\n",
    "Note: - from the home page you have to go to the Show All Dataset page through code<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Chrome webdriver\n",
    "driver = webdriver.Chrome('chromedriver.exe')\n",
    "# opening the url\n",
    "driver.get('https://archive.ics.uci.edu/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening view all dataset page\n",
    "driver.find_element_by_xpath('/html/body/table[1]/tbody/tr/td[2]/span[2]/a').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating variables to store the data\n",
    "name = []\n",
    "data_type = []\n",
    "task = []\n",
    "attribute = []\n",
    "instance = []\n",
    "n_attribute = []\n",
    "year = []\n",
    "\n",
    "#scraping the dataset name\n",
    "for i in driver.find_elements_by_xpath('/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[1]'):\n",
    "    name.append(i.text)\n",
    "    \n",
    "#scraping the data type\n",
    "for j in driver.find_elements_by_xpath('/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[2]'):\n",
    "    data_type.append(j.text)\n",
    "    \n",
    "#scraping the task\n",
    "for k in driver.find_elements_by_xpath('/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[3]'):\n",
    "    task.append(k.text)\n",
    "    \n",
    "#scraping the attribute type\n",
    "for l in driver.find_elements_by_xpath('/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[4]'):\n",
    "    attribute.append(l.text)\n",
    "    \n",
    "#scraping the number of instance\n",
    "for m in driver.find_elements_by_xpath('/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[5]'):\n",
    "    instance.append(m.text)\n",
    "    \n",
    "#scraping the number of attribute\n",
    "for n in driver.find_elements_by_xpath('/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[6]'):\n",
    "    n_attribute.append(n.text)\n",
    "    \n",
    "#scraping the year\n",
    "for o in driver.find_elements_by_xpath('/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[7]'):\n",
    "    year.append(o.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the dataframe\n",
    "Datasets = pd.DataFrame({'Dataset Name': name,\n",
    "                        'Data Type': data_type,\n",
    "                        'Task': task,\n",
    "                        'Attribute Type': attribute,\n",
    "                        'Number of Instance': instance,\n",
    "                        'Number of Attribute': n_attribute,\n",
    "                        'Year': year})\n",
    "Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
